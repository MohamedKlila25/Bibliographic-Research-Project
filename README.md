# Large Language Models (LLMs) Research Project

## Project Overview
This project is a collaborative bibliographic research conducted with my colleague Mohamed Eladab as part of our academic work at ENSTA Paris. The goal of this project is to explore the evolution of large language models (LLMs), their inner workings, and practical techniques to utilize them effectively.

During this project, we studied:
- How language models evolve over time.
- Tokenization processes and how transformers work.
- Techniques for fine-tuning pre-trained models.
- How large language models can learn through reinforcement learning.

## Key Learnings
Through this project, I developed a solid understanding of LLMs, including their architecture and training methods. I also gained valuable experience in collaborative work, research methodology, and technical writing.

## Skills Applied
- Natural Language Processing (NLP) concepts
- Machine Learning & Deep Learning foundations
- Working with transformer models
- Collaborative research and documentation

## Collaboration
This project was conducted in a team of two. Working closely with a partner enhanced our problem-solving skills and helped us gain practical experience in teamwork.


## Notes
This repository includes:
- Research notes and summaries
- References and bibliographic sources
- Supporting documentation
